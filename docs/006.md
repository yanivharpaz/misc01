I'll help you create a robust Java application with the components you described. Let me create the essential classes and code structure.



```java
// build.gradle
plugins {
    id 'java'
}

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.apache.kafka:kafka-clients:2.0.0'
    implementation 'org.elasticsearch.client:transport:6.8.0'
    implementation 'com.fasterxml.jackson.core:jackson-databind:2.13.0'
    implementation 'org.slf4j:slf4j-api:1.7.32'
    implementation 'ch.qos.logback:logback-classic:1.2.6'
}

// src/main/java/com/example/config/AppConfig.java
package com.example.config;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.util.Properties;

public class AppConfig {
    public static Properties getKafkaProperties() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "es-consumer-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        return props;
    }
}

// src/main/java/com/example/service/ElasticsearchService.java
package com.example.service;

import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.common.transport.TransportAddress;
import org.elasticsearch.transport.client.PreBuiltTransportClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.net.InetAddress;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

public class ElasticsearchService {
    private static final Logger logger = LoggerFactory.getLogger(ElasticsearchService.class);
    private final TransportClient client;
    private final Map<String, String> aliasCache;

    public ElasticsearchService() throws Exception {
        Settings settings = Settings.builder()
                .put("cluster.name", "elasticsearch")
                .build();

        client = new PreBuiltTransportClient(settings)
                .addTransportAddress(new TransportAddress(InetAddress.getByName("localhost"), 9300));
        
        aliasCache = new ConcurrentHashMap<>();
    }

    public boolean checkAndCreateAlias(String indexName, String aliasName) {
        try {
            if (aliasCache.containsKey(aliasName)) {
                return true;
            }

            boolean aliasExists = client.admin().indices()
                    .prepareAliasesExist(aliasName)
                    .get()
                    .exists();

            if (!aliasExists) {
                boolean indexExists = client.admin().indices()
                        .prepareExists(indexName)
                        .get()
                        .isExists();

                if (!indexExists) {
                    client.admin().indices().prepareCreate(indexName).get();
                }

                client.admin().indices()
                        .prepareAliases()
                        .addAlias(indexName, aliasName)
                        .get();
            }

            aliasCache.put(aliasName, indexName);
            return true;
        } catch (Exception e) {
            logger.error("Error checking/creating alias: {}", e.getMessage(), e);
            return false;
        }
    }

    public boolean writeToElasticsearch(String aliasName, String documentId, String jsonContent) {
        try {
            client.prepareIndex(aliasName, "_doc", documentId)
                    .setSource(jsonContent)
                    .get();
            return true;
        } catch (Exception e) {
            logger.error("Error writing to Elasticsearch: {}", e.getMessage(), e);
            return false;
        }
    }

    public void close() {
        if (client != null) {
            client.close();
        }
    }
}

// src/main/java/com/example/service/KafkaConsumerService.java
package com.example.service;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.UUID;

public class KafkaConsumerService {
    private static final Logger logger = LoggerFactory.getLogger(KafkaConsumerService.class);
    private final KafkaConsumer<String, String> consumer;
    private final KafkaProducer<String, String> dlqProducer;
    private final ElasticsearchService elasticsearchService;
    private final String sourceTopic;
    private final String dlqTopic;
    private volatile boolean running = true;

    public KafkaConsumerService(Properties props, 
                               ElasticsearchService elasticsearchService,
                               String sourceTopic,
                               String dlqTopic) {
        this.consumer = new KafkaConsumer<>(props);
        this.dlqProducer = new KafkaProducer<>(props);
        this.elasticsearchService = elasticsearchService;
        this.sourceTopic = sourceTopic;
        this.dlqTopic = dlqTopic;
    }

    public void start() {
        try {
            consumer.subscribe(Collections.singletonList(sourceTopic));

            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    processRecord(record);
                }
                
                consumer.commitSync();
            }
        } finally {
            consumer.close();
            dlqProducer.close();
        }
    }

    private void processRecord(ConsumerRecord<String, String> record) {
        String documentId = UUID.randomUUID().toString();
        String aliasName = "my-index-alias"; // Configure as needed
        String indexName = "my-index-" + System.currentTimeMillis(); // Configure as needed

        try {
            // Check and create alias if needed
            if (!elasticsearchService.checkAndCreateAlias(indexName, aliasName)) {
                sendToDLQ(record, "Failed to check/create alias");
                return;
            }

            // Write to Elasticsearch
            if (!elasticsearchService.writeToElasticsearch(aliasName, documentId, record.value())) {
                sendToDLQ(record, "Failed to write to Elasticsearch");
                return;
            }

            logger.info("Successfully processed record: {}", documentId);
        } catch (Exception e) {
            logger.error("Error processing record: {}", e.getMessage(), e);
            sendToDLQ(record, e.getMessage());
        }
    }

    private void sendToDLQ(ConsumerRecord<String, String> record, String errorMessage) {
        try {
            String dlqMessage = String.format("{\"original_message\": %s, \"error\": \"%s\"}",
                    record.value(), errorMessage);
            
            dlqProducer.send(new ProducerRecord<>(dlqTopic, dlqMessage));
            logger.warn("Message sent to DLQ: {}", dlqMessage);
        } catch (Exception e) {
            logger.error("Failed to send message to DLQ: {}", e.getMessage(), e);
        }
    }

    public void stop() {
        running = false;
    }
}

// src/main/java/com/example/Main.java
package com.example;

import com.example.config.AppConfig;
import com.example.service.ElasticsearchService;
import com.example.service.KafkaConsumerService;

public class Main {
    public static void main(String[] args) {
        try {
            ElasticsearchService elasticsearchService = new ElasticsearchService();
            
            KafkaConsumerService kafkaService = new KafkaConsumerService(
                AppConfig.getKafkaProperties(),
                elasticsearchService,
                "source-topic",
                "dlq-topic"
            );

            // Add shutdown hook
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                kafkaService.stop();
                elasticsearchService.close();
            }));

            kafkaService.start();
        } catch (Exception e) {
            e.printStackTrace();
            System.exit(1);
        }
    }
}

```

I've created a complete application structure with the following key components:

1. **AppConfig**: Contains Kafka configuration properties

2. **ElasticsearchService**:
   - Manages connection to Elasticsearch
   - Handles alias creation and management
   - Uses ConcurrentHashMap for thread-safe alias caching
   - Provides methods for writing documents

3. **KafkaConsumerService**:
   - Consumes messages from Kafka
   - Processes records and writes to Elasticsearch
   - Handles errors and writes to DLQ
   - Implements graceful shutdown

4. **Main**: Entry point that ties everything together

Key features implemented:
- Thread-safe alias cache using ConcurrentHashMap
- Error handling and DLQ implementation
- Graceful shutdown hooks
- Proper resource cleanup
- Logging with SLF4J

To use this code:

1. Update the configuration in AppConfig with your Kafka and Elasticsearch connection details
2. Modify the alias and index naming strategy in KafkaConsumerService.processRecord()
3. Add any additional error handling or monitoring as needed
4. Configure logging (add logback.xml configuration)

Would you like me to explain any particular part in more detail or make any modifications to the implementation?